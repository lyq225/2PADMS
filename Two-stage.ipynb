{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72eafa-4cf2-44cd-a971-29b3dbb04f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import Huber\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"your dataset\")\n",
    "\n",
    "df.drop(columns=['Unnamed: 0', 'date', 'serial_number', 'model','capacity_bytes','datacenter','cluster_id', 'vault_id',\n",
    "       'pod_id', 'pod_slot_num', 'is_legacy_format', ], errors='ignore', inplace=True)\n",
    "threshold = len(df) * 0.8\n",
    "\n",
    "df = df.dropna(axis=1,thresh=threshold)\n",
    "df=df.dropna()\n",
    "kcolumns=[\"Selected features\"]\n",
    "dfraw=df[kcolumns]\n",
    "print(dfraw.columns)\n",
    "dfraw.drop(columns=['failure'], errors='ignore', inplace=True)\n",
    "\n",
    "X = dfraw.drop(columns=['label'])\n",
    "y = dfraw['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=3,\n",
    "    reg_lambda=8,\n",
    "    alpha=5,\n",
    "    min_child_weight=3,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    n_estimators=200,\n",
    "    max_depth=22,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = voting_clf.predict(X_test)\n",
    "\n",
    "fault_indices = y_test[y_pred1 == 1].index\n",
    "X_fault = X_test.loc[fault_indices]\n",
    "y_fault = y_test.loc[fault_indices]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "time_steps = 42  \n",
    "n_features = X_scaled.shape[1]\n",
    "features_per_step = n_features  \n",
    "\n",
    "if n_features % features_per_step == 0:\n",
    "    time_steps = min(time_steps, n_features // features_per_step)\n",
    "    features_per_time_step = n_features // time_steps\n",
    "else:\n",
    "\n",
    "    features_per_time_step = n_features // time_steps\n",
    "    if features_per_time_step * time_steps < n_features:\n",
    "        features_per_time_step += 1\n",
    "        padding_needed = features_per_time_step * time_steps - n_features\n",
    "        if padding_needed > 0:\n",
    "            X_scaled = np.pad(X_scaled, ((0, 0), (0, padding_needed)), 'constant')\n",
    "            n_features = X_scaled.shape[1]\n",
    "X_lstm = X_scaled.reshape((X_scaled.shape[0], time_steps, features_per_time_step))\n",
    "\n",
    "print(f\"重塑后的数据形状: {X_lstm.shape}\")\n",
    "\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y, test_size=0.2, random_state=42)\n",
    "\n",
    "inputs = layers.Input(shape=(time_steps, features_per_time_step))\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(512, activation='tanh', return_sequences=True, recurrent_dropout=0.1))(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(256, activation='tanh', return_sequences=True, recurrent_dropout=0.1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(128, activation='tanh', return_sequences=True, recurrent_dropout=0.1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(64, activation='tanh', recurrent_dropout=0.1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def calculate_feature_frequencies(X):\n",
    "    feature_occurrences = np.sum(X != 0, axis=0)\n",
    "    total_samples = X.shape[0]\n",
    "    frequencies = feature_occurrences / total_samples\n",
    "    return frequencies\n",
    "\n",
    "feature_frequencies = calculate_feature_frequencies(X)\n",
    "\n",
    "class RareFeatureWeightedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, original_loss=tf.keras.losses.Huber(delta=1.0), lambda_reg=0.01, epsilon=1e-6, feature_frequencies=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.original_loss = original_loss\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.epsilon = epsilon\n",
    "        self.feature_frequencies = feature_frequencies\n",
    "        \n",
    "        if feature_frequencies is not None:\n",
    "            self.rare_weights = tf.cast(1.0 / (feature_frequencies + self.epsilon), tf.float32)\n",
    "        else:\n",
    "            self.rare_weights = None\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        original_loss_value = self.original_loss(y_true, y_pred)\n",
    "        if self.rare_weights is None:\n",
    "            return original_loss_value\n",
    "        regularization_term = tf.reduce_sum(tf.exp(-self.rare_weights))\n",
    "        \n",
    "        lambda_reg = tf.cast(self.lambda_reg, tf.float32)\n",
    "        \n",
    "        original_loss_value = tf.cast(original_loss_value, tf.float32)\n",
    "        \n",
    "        regularization_term = tf.cast(regularization_term, tf.float32)\n",
    "        \n",
    "        total_loss = original_loss_value + lambda_reg * regularization_term\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "custom_loss = RareFeatureWeightedLoss(\n",
    "    original_loss=tf.keras.losses.Huber(delta=1.0),\n",
    "    lambda_reg=0.01,\n",
    "    epsilon=1e-6,    \n",
    "    feature_frequencies=feature_frequencies\n",
    ")\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=custom_loss, metrics=['mae'])\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-4, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test_lstm, y_test_lstm),\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test_lstm)\n",
    "loss, mae = model.evaluate(X_test_lstm, y_test_lstm)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_lstm, y_pred))\n",
    "\n",
    "print(f\"测试损失: {loss}\")\n",
    "print(f\"测试MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse:.2f} 天\")\n",
    "\n",
    "y_true = np.array(y_test_lstm)\n",
    "\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "predicted_fault_indices = np.where(y_pred_flat <= 5)[0]\n",
    "\n",
    "correct_indices = np.where(np.abs(y_pred_flat - y_true) <= 5)[0]\n",
    "\n",
    "accuracy_5days = len(correct_indices) / len(y_true)\n",
    "\n",
    "if len(predicted_fault_indices) > 0:\n",
    "    precision_5days = len(set(predicted_fault_indices) & set(correct_indices)) / len(predicted_fault_indices)\n",
    "else:\n",
    "    precision_5days = 0\n",
    "\n",
    "print(f\"5天内预测正确的准确率: {accuracy_5days:.4f}\")\n",
    "print(f\"5天内预测正确的精确率: {precision_5days:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"model train loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_lstm, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test_lstm), max(y_test_lstm)], [min(y_test_lstm), max(y_test_lstm)], 'r--')\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"pre\")\n",
    "plt.title(\"model train pre\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "predicted_failure_times = model.predict(X_test_lstm)\n",
    "predicted_failure_times = np.maximum(predicted_failure_times, 0.11)\n",
    "predicted_failure_times = np.round(predicted_failure_times, 2)\n",
    "\n",
    "if len(X_fault) > 0:\n",
    "    print(\"\\n===== 对预测为故障的样本进行分析 =====\")\n",
    "    \n",
    "    fault_original_indices = []\n",
    "    for idx in fault_indices:\n",
    "        original_idx = X.index.get_loc(idx)\n",
    "        fault_original_indices.append(original_idx)\n",
    "    \n",
    "    X_fault_lstm = []\n",
    "    y_fault_actual = []\n",
    "    \n",
    "    for idx in fault_original_indices:\n",
    "        fault_data = X_scaled[idx].reshape(1, -1)\n",
    "        fault_data_reshaped = fault_data.reshape(1, time_steps, features_per_time_step)\n",
    "        X_fault_lstm.append(fault_data_reshaped[0])\n",
    "        y_fault_actual.append(y.iloc[idx])\n",
    "    \n",
    "    if len(X_fault_lstm) > 0:\n",
    "        X_fault_lstm = np.array(X_fault_lstm)\n",
    "        y_fault_actual = np.array(y_fault_actual)     \n",
    "        print(f\"故障样本数量: {len(X_fault_lstm)}\")\n",
    "        y_pred_fault = model.predict(X_fault_lstm)\n",
    "        mae_fault = mean_absolute_error(y_fault_actual, y_pred_fault)\n",
    "        rmse_fault = np.sqrt(mean_squared_error(y_fault_actual, y_pred_fault))\n",
    "        \n",
    "        print(f\"故障样本MAE: {mae_fault:.4f}\")\n",
    "        print(f\"故障样本RMSE: {rmse_fault:.2f} 天\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(y_fault_actual, y_pred_fault, alpha=0.7, color='blue')\n",
    "        plt.plot([min(y_fault_actual), max(y_fault_actual)], \n",
    "                 [min(y_fault_actual), max(y_fault_actual)], 'k--')\n",
    "        plt.xlabel(\"true time\")\n",
    "        plt.ylabel(\"pre time\")\n",
    "        plt.title(\"sample pre\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
